{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea98c39c-a1f4-4122-a149-5f6383fa5808",
   "metadata": {},
   "source": [
    "# Image Segmentation for EyeWear Extraction \n",
    "## Overview\n",
    "This project is developed to segment eyewear from the background and enhance them for display as catalogue images in the website. The algorithm depends on \n",
    "* Yolo-NAS Object Detection Model\n",
    "    * To detect Eyewear, turntable and background\n",
    "* Segment AnyThing Model (Facebook Research)\n",
    "    * To segment the mask out of the Detection boxes\n",
    "* ML model based Image enhancement (CV handcrafted Models)\n",
    "    * Image Enhancement for better Resolution & Sharpness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115749c5-58e0-418a-a204-543e82d6aa01",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Project Setup\n",
    "### Path setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfbe34b5-ee8e-461f-bb22-19a29effd301",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Feb 11 15:25:45 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.154.05             Driver Version: 535.154.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla T4                       Off | 00000001:00:00.0 Off |                  Off |\n",
      "| N/A   43C    P8               9W /  70W |      2MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f805a8e-516c-4abc-8d51-cde3fcfe0db6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Add Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47ece8d2-e7ec-4994-9310-b701571572f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b10cd16-9320-4a73-bc29-c281367fe473",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "OBJ_MODEL_ARCH = 'yolo_nas_l'\n",
    "MODEL_TYPE = \"vit_h\"\n",
    "SAM_MODEL_ARCH = \"sam_vit_h_4b8939.pth\"\n",
    "IMAGE_TYPE = '.jpeg'\n",
    "WORKSPACE = os.getcwd()\n",
    "MODEL_DIR = os.path.join(WORKSPACE, \"../models/pth\")\n",
    "IMAGE_DIR = os.path.join(WORKSPACE, \"../data/NATT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c8ec970-a7d9-486d-8174-20864c15a836",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORKSPACE: /workspace/notebook\n",
      "MODEL DIR: /workspace/notebook/../models/pth\n"
     ]
    }
   ],
   "source": [
    "print(\"WORKSPACE:\", WORKSPACE)\n",
    "print(\"MODEL DIR:\", MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b9a5a6-d232-4bd2-a2e3-44357a460dcb",
   "metadata": {},
   "source": [
    "### Download Necessary Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e022d7b-ea66-43e9-a5e4-20be0fc073cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_PATH = os.path.join(MODEL_DIR, SAM_MODEL_ARCH)\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    !wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth -P {MODEL_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90f03fb-b53b-435d-8a40-d30d39aed3f0",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e482348f-c12b-4147-ae1f-e358636c225b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The console stream is logged into /root/sg_logs/console.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-02-11 15:25:55] INFO - crash_tips_setup.py - Crash tips is enabled. You can set your environment variable to CRASH_HANDLER=FALSE to disable it\n",
      "[2024-02-11 15:25:56] WARNING - __init__.py - Failed to import pytorch_quantization\n",
      "[2024-02-11 15:25:57] WARNING - calibrator.py - Failed to import pytorch_quantization\n",
      "[2024-02-11 15:25:57] WARNING - export.py - Failed to import pytorch_quantization\n",
      "[2024-02-11 15:25:57] WARNING - selective_quantization_utils.py - Failed to import pytorch_quantization\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy\n",
    "from matplotlib import pyplot as plt\n",
    "from super_gradients.training import models\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "from matplotlib import pyplot as plt\n",
    "import supervision as sv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5377b6f-5991-4eb1-9ba9-9cf24c7ec2ce",
   "metadata": {},
   "source": [
    "### Load Models\n",
    "* SAM\n",
    "* OD - YOLO-NAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fd49233-ce1f-40a4-84f6-58ef94ab36ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup Device First\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1871f98-788e-4771-ba41-6f06a124c06d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: '/workspace/notebook/../models/pth/sam_vit_h_4b8939.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load SAM Model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m sam_model \u001b[38;5;241m=\u001b[39m \u001b[43msam_model_registry\u001b[49m\u001b[43m[\u001b[49m\u001b[43mMODEL_TYPE\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODEL_PATH\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mDEVICE)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/segment_anything/build_sam.py:15\u001b[0m, in \u001b[0;36mbuild_sam_vit_h\u001b[0;34m(checkpoint)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_sam_vit_h\u001b[39m(checkpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_build_sam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_embed_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1280\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_num_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_global_attn_indexes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m23\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m31\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/segment_anything/build_sam.py:104\u001b[0m, in \u001b[0;36m_build_sam\u001b[0;34m(encoder_embed_dim, encoder_depth, encoder_num_heads, encoder_global_attn_indexes, checkpoint)\u001b[0m\n\u001b[1;32m    102\u001b[0m sam\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m checkpoint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 104\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    105\u001b[0m         state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m    106\u001b[0m     sam\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict)\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/workspace/notebook/../models/pth/sam_vit_h_4b8939.pth'"
     ]
    }
   ],
   "source": [
    "# Load SAM Model\n",
    "sam_model = sam_model_registry[MODEL_TYPE](checkpoint=MODEL_PATH).to(device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bf25cc-9378-465b-afcf-f086d58424b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "od_model = models.get(OBJ_MODEL_ARCH, pretrained_weights=\"coco\").to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6770cf32-b3dc-40f3-94ca-8335cd8af674",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load test Images from the Test Image Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1263bcd8-787d-4887-bdc5-1ccf14c5a066",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "impaths = list()\n",
    "for root, dirs, files in os.walk(IMAGE_DIR):\n",
    "    for file in files:\n",
    "        if file.endswith(IMAGE_TYPE):\n",
    "            impaths.append(os.path.join(root, file))\n",
    "\n",
    "print(\"{} test images found.\".format(len(impaths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5934eb-cc30-4766-bb12-8dc64ebde06a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "for idx, impath in enumerate(impaths[:8]):\n",
    "    img = cv2.imread(impath)\n",
    "    #Show the image with matplotlib\n",
    "    plt.subplot(4, 2, idx+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c93f21e-e99c-47d1-abb0-fcdaa0ef44ba",
   "metadata": {},
   "source": [
    "## Object Detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e87337e-1467-45ee-98fc-6e5eae32728f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_bgr = cv2.imread(impaths[0])\n",
    "image_bgr = cv2.resize(image_bgr, (1200, 720), interpolation = cv2.INTER_LINEAR)\n",
    "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bb6464-1679-412c-b0a1-306ec6786b67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = list(od_model.predict(image_rgb, conf=0.15))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cc19f7-2bee-411d-870d-f87c4b20e583",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "detections = sv.Detections(\n",
    "    xyxy=result.prediction.bboxes_xyxy,\n",
    "    confidence=result.prediction.confidence,\n",
    "    class_id=result.prediction.labels.astype(int)\n",
    ")\n",
    "\n",
    "box_annotator = sv.BoxAnnotator()\n",
    "\n",
    "labels = [\n",
    "    f\"{result.class_names[class_id]} {confidence:0.2f}\"\n",
    "    for _, _, confidence, class_id, _\n",
    "    in detections\n",
    "]\n",
    "\n",
    "annotated_frame = box_annotator.annotate(\n",
    "    scene=image_rgb.copy(),\n",
    "    detections=detections,\n",
    "    labels=labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbc6071-194e-4ac3-8569-1e8cc998ecc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "sv.plot_image(annotated_frame, (12, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa9fae0-322c-4fd0-a4d8-a03956b973db",
   "metadata": {},
   "source": [
    "## Detection and SAM Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8903812d-16de-4808-84b0-b2a8d68e73bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_bgr = cv2.imread(impaths[0])\n",
    "image_bgr = cv2.resize(image_bgr, (1200, 720), interpolation = cv2.INTER_LINEAR)\n",
    "mask_generator = SamAutomaticMaskGenerator(sam_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dd3e75-1368-4bc4-83fd-81e2dbb2315e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "sam_result = mask_generator.generate(image_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df67bbf6-10d9-4bce-9d98-7b6fc7ce3020",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "for idx, ires in enumerate(sam_result):\n",
    "    start = (sam_result[idx][\"bbox\"][0], sam_result[idx][\"bbox\"][1])\n",
    "    end = (sam_result[idx][\"bbox\"][2], sam_result[idx][\"bbox\"][3])\n",
    "    test_img = cv2.rectangle(image_bgr, start, end, (200, 0, 200), 2)\n",
    "    plt.imshow(test_img)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64668043-b016-4129-9a14-f66ef2c99660",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15486952-d215-4eff-b231-45e8da1e6ff5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a393151-446b-44d3-8074-6da606ffe5d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask_annotator = sv.MaskAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
    "detections = sv.Detections.from_sam(sam_result=sam_result)\n",
    "annotated_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n",
    "\n",
    "sv.plot_images_grid(\n",
    "    images=[image_bgr, annotated_image],\n",
    "    grid_size=(1, 2),\n",
    "    titles=['source image', 'segmented image']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f33675-6f41-415a-a96a-f5de1c4a78ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
